
**Part 1: Scaling VM Cluster Compute Resources (OCPUs, Memory, Local Storage)**

1.  The Exadata Database Service allows you to scale VM cluster resources (compute, memory, local storage) up or down **online** to align with business needs.

2.  To scale, navigate to the **Cluster Details** page and initiate the **Scale VM Cluster** action.

3.  The scale workflow shows the number of VMs and the current resource allocation **per VM**.

4.  You can change the allocation **per VM** for the following resources:
    *   OCPUs
    *   Memory
    *   Local File System Storage

5.  When scaling up, consider how many resources to leave free for future VM clusters you might create.

6.  The total resource allocation across all VMs in the cluster is displayed as a read-only value.

**Part 2: Important Notes on Scaling Compute Resources**

7.  **Billing:** Charges for OCPU utilization are independent from the charges for the underlying Exadata Cloud Infrastructure.

8.  **OCPU Scaling:**
    *   The number of OCPUs specified in the UI is **per VM**.
    *   You can scale down to **zero-enabled OCPUs**, which shuts down all VM cluster compute servers. In this state, you are only billed for the infrastructure.
    *   To restart, scale the OCPUs back up with a **minimum of two OCPUs per VM**.
    *   If using Database Resource Manager (instance caging), you must explicitly alter the `CPU_COUNT` parameter to leverage additional cores.

9.  **Memory Scaling:**
    *   The minimum memory is **30 GB per VM**.
    *   The value must be a multiple of 1 GB and is limited by the available infrastructure memory.
    *   Memory changes cause associated compute servers to be rebooted in a **rolling manner** to minimize impact.
    *   Changes of less than **2%** from the current value are ignored to prevent unnecessary disruptions.

10. **Local File System Storage Scaling:**
    *   The minimum size is **60 GB per VM**.
    *   The value must be a multiple of 1 GB and is limited by the available infrastructure storage.
    *   Storage size changes also cause a **rolling reboot** of associated servers.
    *   Changes of less than **2%** are ignored.

**Part 3: Allocating Exadata Storage to a VM Cluster**

11. In the **Configure Exadata Storage** section, you specify the **total usable Exadata storage** allocated to the VM cluster.

12. The minimum allocation is **2 TB**, with larger allocations made in increments of **1 TB**.

13. Storage is allocated evenly from all available Exadata storage servers.

14. You can **reduce** the allocated storage to free up space for other clusters, but the new size must be large enough to cover existing data and allow for future growth.

**Part 4: Multi-VM and Node Subsetting (Creating Multiple VM Clusters)**

15. **Multi-VM** is the ability to create multiple VM clusters on a single Exadata infrastructure.

16. **Benefits of Multi-VM:**
    *   **Data Security Separation:** Deploy separate clusters for different environments (e.g., Dev, Test, Prod) or to meet compliance requirements (e.g., PCI, HIPAA).
    *   **Network Isolation:** Each VM cluster can use distinct VLAN tag networks.
    *   **Minimized Maintenance Impact:** Isolate groups of databases to coordinate maintenance windows more easily.
    *   **Failure Isolation:** Limit the impact of a component failure to a single cluster.
    *   **Tailored Resource Management:** Prevent the "noisy neighbor" issue and apply specific resource plans to each cluster.
    *   **Administrative Separation:** Different administration teams can manage their own clusters.

17. **VM Cluster Node Subsetting** is the ability to create a VM cluster on only a **subset** of the database servers in the Exadata infrastructure, and to expand/shrink the cluster by adding or removing nodes.

18. **Benefits of Node Subsetting:**
    *   **Right-Sizing:** Allocate the exact resources needed for specific workloads.
    *   **Efficient Resource Sharing:** Isolate or co-locate workloads on appropriately sized clusters (e.g., a small cluster for low-resource databases, a larger one for critical workloads).
    *   **Cost Efficiency:** You only pay for enabled OCPUs on the subset of VMs used by the cluster.
    *   **BYOL Savings:** Apply BYOL licenses only to the OCPUs in the subset, which is beneficial if clusters use different database features.

**Part 5: How to Provision a New VM Cluster with Node Subsetting**

19. Navigate to the **Exadata Infrastructure Details** page and initiate the **Create VM Cluster** workflow.

20. In the **Configure VM Cluster** section, click **Change DB Servers** to launch the VM placement selection screen.

21. On the **Change DB Server** screen, select the specific DB servers to host the new cluster's VMs.
    *   Each server shows its available resources (OCPU, memory, local storage) and any existing VM clusters already running on it.

22. Based on isolation/co-location preferences and resource limits, choose the suitable DB servers and click **Save Changes**.

23. Then, specify the resource allocation **per VM** (OCPUs, memory, local storage).

24. Complete the remaining creation details and click **Create Exadata VM Cluster**.

**Part 6: Expanding or Shrinking a VM Cluster (Adding/Removing VMs)**

25. Once provisioned, you can expand or shrink a VM cluster by adding or removing VMs.

26. To **shrink** a cluster (remove a VM):
    *   Navigate to the cluster details page.
    *   Find the specific VM you want to remove in the list of VMs.
    *   Click the **Action** menu (three dots) at the end of that VM's row.
    *   Select the option to **Terminate** the VM.
    *   **Confirm** the action by entering the VM's name and clicking **Remove**.

27. **Important Note:** Deleting a VM from the cluster will **terminate any database instances** running on that VM.
